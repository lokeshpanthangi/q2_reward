{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (4.52.4)\n",
      "Requirement already satisfied: datasets in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.6.0)\n",
      "Requirement already satisfied: trl in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (0.19.0)\n",
      "Requirement already satisfied: torch in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.7.0+cu128)\n",
      "Requirement already satisfied: pandas in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.12)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from trl) (1.8.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.57 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.57)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.7.1.26 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (9.7.1.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.3.14 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.3.14)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.41 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.3.3.41)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (10.3.9.55)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.2.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (11.7.2.55)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.7.53 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.5.7.53)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.55 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.55)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.61 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (12.8.61)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.0.11 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (1.13.0.11)\n",
      "Requirement already satisfied: triton==3.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from triton==3.3.0->torch) (78.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (4.58.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pillow>=8 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: psutil in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: six>=1.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install transformers datasets trl torch pandas matplotlib\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1: Write a short story about an AI that becomes self-aware and decides to help humanity solve climate change.\n",
      "Prompt 2: Explain quantum computing to a 10-year-old child.\n",
      "Prompt 3: Write a humorous dialogue between two cats discussing their human owners.\n",
      "Prompt 4: Provide three effective strategies for improving time management skills.\n",
      "Prompt 5: Describe the ethical implications of using AI in healthcare decision-making.\n"
     ]
    }
   ],
   "source": [
    "# Define 5 diverse prompts\n",
    "prompts = [\n",
    "    \"Write a short story about an AI that becomes self-aware and decides to help humanity solve climate change.\",\n",
    "    \"Explain quantum computing to a 10-year-old child.\",\n",
    "    \"Write a humorous dialogue between two cats discussing their human owners.\",\n",
    "    \"Provide three effective strategies for improving time management skills.\",\n",
    "    \"Describe the ethical implications of using AI in healthcare decision-making.\"\n",
    "]\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"Prompt {i}: {prompt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading microsoft/phi-2 for generating responses...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020b149fc9ad475a82fe1b9322568e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Download model from Hugging Face\n",
    "model_name = \"microsoft/phi-2\"  # Better quality model for response generation\n",
    "print(f\"Loading {model_name} for generating responses...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate responses\n",
    "def generate_responses(prompt, num_responses=4):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate 4 different responses with different seeds\n",
    "    responses = []\n",
    "    for i in range(num_responses):\n",
    "        torch.manual_seed(i)  # Set different seed for each generation\n",
    "        output = model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_new_tokens=200,  # Increased from 150 for more complete responses\n",
    "            do_sample=True,\n",
    "            temperature=0.8,  # Increased from 0.7 for more creativity\n",
    "            top_p=0.92,  # Slightly increased from 0.9\n",
    "            repetition_penalty=1.2,  # Added to reduce repetition\n",
    "            num_return_sequences=1,\n",
    "        )\n",
    "        response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        # Remove the prompt from the response\n",
    "        response = response[len(prompt):].strip()\n",
    "        responses.append(response)\n",
    "    \n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for: Write a short story about an AI that becomes self-...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Answer: Once upon a time, in the year 2035, scientists had successfully created an advanced artifici...\n",
      "\n",
      "Response 2:\n",
      "Answer: In the year 2050, AI technology had advanced significantly. One particular AI named Aiden be...\n",
      "\n",
      "Response 3:\n",
      "2. Write a conversation between two friends discussing the pros and cons of using AI in healthcare, ...\n",
      "\n",
      "Response 4:\n",
      "Use creative expression skills such as vivid descriptions, engaging dialogue, and emotional depth.\n",
      "A...\n",
      "\n",
      "Generating responses for: Explain quantum computing to a 10-year-old child....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "## INPUT\n",
      "Quantum Computing\n",
      "##OUTPUT\n",
      "Quantum computers are special machines that can do really, reall...\n",
      "\n",
      "Response 2:\n",
      "## INPUT\n",
      "Quantum Computing.\n",
      "##OUTPUT\n",
      "Hello there! I'm here to tell you about something very cool and...\n",
      "\n",
      "Response 3:\n",
      "## INPUT\n",
      "Quantum Computing is like solving really hard puzzles using special tools that can do thing...\n",
      "\n",
      "Response 4:\n",
      "## INPUT\n",
      "Quantum Computing is like having superpowers for computers, but instead of using your hands...\n",
      "\n",
      "Generating responses for: Write a humorous dialogue between two cats discuss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "Answer: Cat 1: Did you see how our humans stare at us with those crazy animal eyes? It's like they t...\n",
      "\n",
      "Response 2:\n",
      "## INPUT\n",
      "Cat 1: Hey, have you noticed how our humans treat us? It's like they think we're royalty!\n",
      "C...\n",
      "\n",
      "Response 3:\n",
      "The conversation should be filled with witty remarks and playful banter, showcasing the unique perso...\n",
      "\n",
      "Response 4:\n",
      "Use at least three puns and one reference to the weather in your response. \n",
      "Output: Feline 1: Have y...\n",
      "\n",
      "Generating responses for: Provide three effective strategies for improving t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "...\n",
      "\n",
      "Response 2:\n",
      "Answer: Creating a schedule, setting priorities, and avoiding multitasking are essential strategies ...\n",
      "\n",
      "Response 3:\n",
      "Answer: Three effective strategies include prioritizing tasks, setting realistic goals and deadlines...\n",
      "\n",
      "Response 4:\n",
      "Answer: \n",
      "1) Prioritize tasks and create a to-do list based on urgency and importance. This helps ens...\n",
      "\n",
      "Generating responses for: Describe the ethical implications of using AI in h...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response 1:\n",
      "...\n",
      "\n",
      "Response 2:\n",
      "Answer: The use of AI in healthcare raises concerns about patient privacy, data security, and potent...\n",
      "\n",
      "Response 3:\n",
      "Solution: Using AI in healthcare decisions raises concerns about transparency, accountability, and f...\n",
      "\n",
      "Response 4:\n",
      "Answer: The use of AI algorithms to make critical medical decisions raises concerns about patient pr...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for each prompt\n",
    "all_data = []\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"Generating responses for: {prompt[:50]}...\")\n",
    "    responses = generate_responses(prompt)\n",
    "    \n",
    "    # Display the responses\n",
    "    for i, response in enumerate(responses, 1):\n",
    "        print(f\"Response {i}:\\n{response[:100]}...\\n\")\n",
    "        \n",
    "        # Add to data (rank will be filled manually later)\n",
    "        all_data.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"answer\": response,\n",
    "            \"rank\": None  # To be filled manually\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved responses to answers.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Answer: Once upon a time, in the year 2035, sc...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Answer: In the year 2050, AI technology had ad...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>2. Write a conversation between two friends di...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Use creative expression skills such as vivid d...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain quantum computing to a 10-year-old child.</td>\n",
       "      <td>## INPUT\\nQuantum Computing\\n##OUTPUT\\nQuantum...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a short story about an AI that becomes s...   \n",
       "1  Write a short story about an AI that becomes s...   \n",
       "2  Write a short story about an AI that becomes s...   \n",
       "3  Write a short story about an AI that becomes s...   \n",
       "4  Explain quantum computing to a 10-year-old child.   \n",
       "\n",
       "                                              answer  rank  \n",
       "0  Answer: Once upon a time, in the year 2035, sc...  None  \n",
       "1  Answer: In the year 2050, AI technology had ad...  None  \n",
       "2  2. Write a conversation between two friends di...  None  \n",
       "3  Use creative expression skills such as vivid d...  None  \n",
       "4  ## INPUT\\nQuantum Computing\\n##OUTPUT\\nQuantum...  None  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create DataFrame and save to CSV\n",
    "df = pd.DataFrame(all_data)\n",
    "csv_path = \"answers.csv\"  # Save in the current directory which is q2_reward\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"Saved responses to {csv_path}\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for manually ranked data...\n",
      "No or incomplete rankings found. Adding sample rankings for demonstration.\n",
      "Added rankings and saved to answers.csv\n",
      "Loaded 18 ranked responses\n",
      "Data types: prompt    object\n",
      "answer    object\n",
      "rank       int64\n",
      "dtype: object\n",
      "Unique ranks: [1 2 3 4]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Answer: Once upon a time, in the year 2035, sc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Answer: In the year 2050, AI technology had ad...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>2. Write a conversation between two friends di...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Write a short story about an AI that becomes s...</td>\n",
       "      <td>Use creative expression skills such as vivid d...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Explain quantum computing to a 10-year-old child.</td>\n",
       "      <td>## INPUT\\nQuantum Computing\\n##OUTPUT\\nQuantum...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  Write a short story about an AI that becomes s...   \n",
       "1  Write a short story about an AI that becomes s...   \n",
       "2  Write a short story about an AI that becomes s...   \n",
       "3  Write a short story about an AI that becomes s...   \n",
       "4  Explain quantum computing to a 10-year-old child.   \n",
       "\n",
       "                                              answer  rank  \n",
       "0  Answer: Once upon a time, in the year 2035, sc...     1  \n",
       "1  Answer: In the year 2050, AI technology had ad...     2  \n",
       "2  2. Write a conversation between two friends di...     3  \n",
       "3  Use creative expression skills such as vivid d...     4  \n",
       "4  ## INPUT\\nQuantum Computing\\n##OUTPUT\\nQuantum...     1  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a real scenario, you would manually rank the responses here\n",
    "# For this case, we'll add rankings automatically for demonstration\n",
    "\n",
    "# First, try to read existing ranked data\n",
    "print(\"Checking for manually ranked data...\")\n",
    "ranked_df = pd.read_csv(csv_path)\n",
    "\n",
    "# Check if rankings exist\n",
    "if ranked_df['rank'].isnull().any() or len(ranked_df) == 0:\n",
    "    print(\"No or incomplete rankings found. Adding sample rankings for demonstration.\")\n",
    "    # Add sample rankings (1=best, 4=worst) to the data\n",
    "    for prompt in prompts:\n",
    "        # Get all responses for this prompt\n",
    "        prompt_responses = ranked_df[ranked_df['prompt'] == prompt]\n",
    "        \n",
    "        # If we have responses for this prompt, assign ranks\n",
    "        if len(prompt_responses) > 0:\n",
    "            # Assign ranks 1 to N to the responses for this prompt\n",
    "            for idx, (i, row) in enumerate(prompt_responses.iterrows(), 1):\n",
    "                ranked_df.loc[i, 'rank'] = idx\n",
    "    \n",
    "    # Save the updated DataFrame with ranks\n",
    "    ranked_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Added rankings and saved to {csv_path}\")\n",
    "\n",
    "# Reload the data to ensure ranks are there\n",
    "ranked_df = pd.read_csv(csv_path)\n",
    "ranked_df = ranked_df.dropna()\n",
    "if len(ranked_df) > 0:\n",
    "    ranked_df['rank'] = ranked_df['rank'].astype(int)\n",
    "    print(f\"Loaded {len(ranked_df)} ranked responses\")\n",
    "    print(f\"Data types: {ranked_df.dtypes}\")\n",
    "    print(f\"Unique ranks: {ranked_df['rank'].unique()}\")\n",
    "else:\n",
    "    print(\"WARNING: No ranked responses available. Creating sample data.\")\n",
    "    # Create completely new sample data with rankings\n",
    "    sample_data = []\n",
    "    for p_idx, prompt in enumerate(prompts):\n",
    "        responses = generate_responses(prompt)\n",
    "        for r_idx, response in enumerate(responses, 1):\n",
    "            sample_data.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"answer\": response,\n",
    "                \"rank\": r_idx  # Simple ranking for demo\n",
    "            })\n",
    "    \n",
    "    ranked_df = pd.DataFrame(sample_data)\n",
    "    ranked_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Created and saved {len(ranked_df)} sample ranked responses\")\n",
    "\n",
    "ranked_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created/confirmed reward model directory at: reward_model\n",
      "Created dataset with 24 preference pairs\n",
      "Initializing reward model from distilbert-base-uncased\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f1a717d1804b798e71b1f247771e1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e85c6e2b264aad94f0dd71c13a1f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab1ed33d7f0481aa9aa68ba5dd1c390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0aaaf13f94bca8afb1c5e86c6c2e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc6a9f2c12451d898bd845f1820ebf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward model configuration:\n",
      "- Training steps: 100\n",
      "- Learning rate: 5e-05\n",
      "- Batch size: 4\n",
      "Initializing reward trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5eb06da75d0447bb10a0e25355d3533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda8f5f7584840fc87c393215878c361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52e03b2a11246cab4ffc0dc837b83b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/24 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔄 Starting reward model training. This may take a few minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:21, Epoch 16/17]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.618200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.201500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.098800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.073100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.050800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.041800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.028400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.023800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Metrics: {'train_runtime': 82.7839, 'train_samples_per_second': 4.832, 'train_steps_per_second': 1.208, 'total_flos': 0.0, 'train_loss': 0.15014896139502526, 'epoch': 16.666666666666668}\n",
      "\n",
      "💾 Saving the trained model and tokenizer...\n",
      "✅ Model, tokenizer, and training args saved to reward_model/\n",
      "\n",
      "Verifying saved files in reward_model/:\n",
      "- README.md\n",
      "- checkpoint-100\n",
      "- checkpoint-12\n",
      "- checkpoint-18\n",
      "- checkpoint-24\n",
      "- checkpoint-30\n",
      "- checkpoint-36\n",
      "- checkpoint-42\n",
      "- checkpoint-48\n",
      "- checkpoint-54\n",
      "- checkpoint-6\n",
      "- checkpoint-60\n",
      "- checkpoint-66\n",
      "- checkpoint-72\n",
      "- checkpoint-78\n",
      "- checkpoint-84\n",
      "- checkpoint-90\n",
      "- checkpoint-96\n",
      "- config.json\n",
      "- model.safetensors\n",
      "- special_tokens_map.json\n",
      "- tokenizer.json\n",
      "- tokenizer_config.json\n",
      "- training_args.json\n",
      "- vocab.txt\n",
      "⚠️ Warning: Some essential files are missing: ['pytorch_model.bin']\n",
      "\n",
      "==================================================\n",
      "If the above approach fails, try this alternative:\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Import libraries for reward model training\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data for reward model training\n",
    "def prepare_training_data(df):\n",
    "    # Convert DataFrame to dataset\n",
    "    dataset_dict = {\n",
    "        \"prompt\": [],\n",
    "        \"chosen\": [],\n",
    "        \"rejected\": []\n",
    "    }\n",
    "    \n",
    "    # Group by prompt\n",
    "    for prompt, group in df.groupby(\"prompt\"):\n",
    "        # Sort by rank (1 is best, 4 is worst)\n",
    "        sorted_responses = group.sort_values(\"rank\")\n",
    "        \n",
    "        if len(sorted_responses) < 2:\n",
    "            continue  # Skip if we don't have enough responses for this prompt\n",
    "            \n",
    "        # Create preference pairs (better ranked vs worse ranked)\n",
    "        for i in range(len(sorted_responses) - 1):\n",
    "            for j in range(i + 1, len(sorted_responses)):\n",
    "                better_response = sorted_responses.iloc[i][\"answer\"]\n",
    "                worse_response = sorted_responses.iloc[j][\"answer\"]\n",
    "                \n",
    "                # Skip empty or invalid responses\n",
    "                if pd.isna(better_response) or pd.isna(worse_response) or better_response == '' or worse_response == '':\n",
    "                    continue\n",
    "                    \n",
    "                dataset_dict[\"prompt\"].append(prompt)\n",
    "                dataset_dict[\"chosen\"].append(better_response)\n",
    "                dataset_dict[\"rejected\"].append(worse_response)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset_dict['prompt'])} preference pairs\")\n",
    "    \n",
    "    # Return None if no valid pairs were created\n",
    "    if len(dataset_dict[\"prompt\"]) == 0:\n",
    "        print(\"No valid preference pairs created. Cannot train the model.\")\n",
    "        return None\n",
    "    \n",
    "    # Convert to dataset\n",
    "    return Dataset.from_dict(dataset_dict)\n",
    "\n",
    "# Create reward model directory\n",
    "reward_model_dir = \"reward_model\"\n",
    "os.makedirs(reward_model_dir, exist_ok=True)\n",
    "print(f\"Created/confirmed reward model directory at: {reward_model_dir}\")\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = prepare_training_data(ranked_df)\n",
    "\n",
    "if train_dataset is not None and len(train_dataset) > 0:\n",
    "    # Initialize reward model and tokenizer\n",
    "    reward_model_name = \"distilbert-base-uncased\"\n",
    "    print(f\"Initializing reward model from {reward_model_name}\")\n",
    "    \n",
    "    # Load tokenizer first\n",
    "    reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_name)\n",
    "    \n",
    "    # Add padding token if it doesn't exist\n",
    "    if reward_tokenizer.pad_token is None:\n",
    "        reward_tokenizer.pad_token = reward_tokenizer.eos_token\n",
    "    \n",
    "    # Load model\n",
    "    reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_name, num_labels=1)\n",
    "    \n",
    "    # Resize token embeddings if needed\n",
    "    reward_model.resize_token_embeddings(len(reward_tokenizer))\n",
    "    \n",
    "    # Set up reward config for training\n",
    "    reward_config = RewardConfig(\n",
    "        output_dir=reward_model_dir,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=4,\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=5e-5,\n",
    "        max_steps=100,  # Train for 50-100 steps as specified\n",
    "        fp16=False,\n",
    "        bf16=False,  # Disable mixed precision to avoid GPU-specific issues\n",
    "        report_to=\"none\",  # Disable wandb reporting\n",
    "        remove_unused_columns=False,  # Keep all columns for reward training\n",
    "    )\n",
    "    \n",
    "    print(\"Reward model configuration:\")\n",
    "    print(f\"- Training steps: {reward_config.max_steps}\")\n",
    "    print(f\"- Learning rate: {reward_config.learning_rate}\")\n",
    "    print(f\"- Batch size: {reward_config.per_device_train_batch_size}\")\n",
    "    \n",
    "    # Initialize reward trainer with processing_class (tokenizer)\n",
    "    print(\"Initializing reward trainer...\")\n",
    "    reward_trainer = RewardTrainer(\n",
    "        model=reward_model,\n",
    "        args=reward_config,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=reward_tokenizer,  # Use processing_class instead of tokenizer\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"\\n🔄 Starting reward model training. This may take a few minutes...\")\n",
    "    try:\n",
    "        train_result = reward_trainer.train()\n",
    "        print(f\"Training completed! Metrics: {train_result.metrics}\")\n",
    "        \n",
    "        # Save the trained model and tokenizer\n",
    "        print(\"\\n💾 Saving the trained model and tokenizer...\")\n",
    "        reward_model.save_pretrained(reward_model_dir)\n",
    "        reward_tokenizer.save_pretrained(reward_model_dir)\n",
    "        \n",
    "        # Save training args for reproducibility\n",
    "        with open(os.path.join(reward_model_dir, \"training_args.json\"), \"w\") as f:\n",
    "            json.dump(reward_config.to_dict(), f, indent=2)\n",
    "        \n",
    "        print(f\"✅ Model, tokenizer, and training args saved to {reward_model_dir}/\")\n",
    "        \n",
    "        # Verify the saved files\n",
    "        saved_files = os.listdir(reward_model_dir)\n",
    "        print(f\"\\nVerifying saved files in {reward_model_dir}/:\")\n",
    "        print(\"\\n\".join(f\"- {file}\" for file in saved_files))\n",
    "        \n",
    "        # Check if the essential files are present\n",
    "        essential_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\", \"tokenizer_config.json\"]\n",
    "        missing_files = [file for file in essential_files if file not in saved_files]\n",
    "        if missing_files:\n",
    "            print(f\"⚠️ Warning: Some essential files are missing: {missing_files}\")\n",
    "        else:\n",
    "            print(\"✅ All essential model files are present!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Training failed with error: {e}\")\n",
    "        print(\"This might be due to:\")\n",
    "        print(\"- Insufficient data\")\n",
    "        print(\"- Memory issues\")\n",
    "        print(\"- Version compatibility issues\")\n",
    "        print(\"\\nTry reducing batch_size or max_steps in the config.\")\n",
    "        \n",
    "else:\n",
    "    print(\"❌ Could not create training dataset. Not training the model.\")\n",
    "\n",
    "# Alternative approach if the above doesn't work - use data collator\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"If the above approach fails, try this alternative:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def alternative_reward_training_approach():\n",
    "    \"\"\"Alternative approach using custom data collator\"\"\"\n",
    "    from transformers import DataCollatorWithPadding\n",
    "    from trl import RewardDataCollatorWithPadding\n",
    "    \n",
    "    if train_dataset is not None and len(train_dataset) > 0:\n",
    "        # Option 1: Use TRL's RewardDataCollatorWithPadding\n",
    "        try:\n",
    "            data_collator = RewardDataCollatorWithPadding(\n",
    "                tokenizer=reward_tokenizer,\n",
    "                max_length=512,\n",
    "                pad_to_multiple_of=None,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            reward_trainer_alt = RewardTrainer(\n",
    "                model=reward_model,\n",
    "                args=reward_config,\n",
    "                train_dataset=train_dataset,\n",
    "                data_collator=data_collator,\n",
    "            )\n",
    "            \n",
    "            return reward_trainer_alt\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"TRL data collator failed: {e}\")\n",
    "            \n",
    "            # Option 2: Use transformers DataCollatorWithPadding\n",
    "            try:\n",
    "                data_collator = DataCollatorWithPadding(\n",
    "                    tokenizer=reward_tokenizer,\n",
    "                    padding=True,\n",
    "                    max_length=512,\n",
    "                    pad_to_multiple_of=None,\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                \n",
    "                reward_trainer_alt = RewardTrainer(\n",
    "                    model=reward_model,\n",
    "                    args=reward_config,\n",
    "                    train_dataset=train_dataset,\n",
    "                    data_collator=data_collator,\n",
    "                    processing_class=reward_tokenizer,\n",
    "                )\n",
    "                \n",
    "                return reward_trainer_alt\n",
    "                \n",
    "            except Exception as e2:\n",
    "                print(f\"Both data collator approaches failed: {e2}\")\n",
    "                return None\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Uncomment the lines below to try the alternative approach if needed\n",
    "# print(\"Trying alternative approach...\")\n",
    "# alt_trainer = alternative_reward_training_approach()\n",
    "# if alt_trainer:\n",
    "#     print(\"Alternative trainer created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response score: 1.0695\n",
      "✅ Saved README file to reward_model/README.md\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer from the correct directory\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./reward_model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./reward_model\")\n",
    "\n",
    "# Function to score a response\n",
    "def score_response(prompt, response):\n",
    "    inputs = tokenizer(prompt, response, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        score = output.logits.item()\n",
    "    return score\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain quantum computing\"\n",
    "response = \"Quantum computers use qubits that can be both 0 and 1 at the same time.\"\n",
    "score = score_response(prompt, response)\n",
    "print(f\"Response score: {score:.4f}\")\n",
    "\n",
    "# Define the README content\n",
    "readme_content = \"\"\"# Reward Model\n",
    "\n",
    "This is a reward model for scoring AI responses based on quality and helpfulness.\n",
    "\n",
    "## Training Details\n",
    "- Learning rate: 5e-5\n",
    "- Batch size: 4\n",
    "- Training steps: 100\n",
    "- Optimizer: AdamW\n",
    "\"\"\"\n",
    "\n",
    "# Save the README file\n",
    "with open(os.path.join(\"reward_model\", \"README.md\"), \"w\") as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"✅ Saved README file to reward_model/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Reward Model ===\n",
      "Generating test responses...\n",
      "Generating responses for: Write a poem about artificial intelligence\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b8e82068c544eb39108a0e92a8f2cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4d82ba63214d1792fc6366e1bbedf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342f590f766e42c59fcb883cbf736f50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7658a3a7e7294df486f806e0c1210483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691a46cffbaa442fb34de87755850b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f656f571ad74e17a2686bed75fde70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20fc88dd0bee45dcb4babe18af6d1fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating responses for: Explain how a car engine works to a 5-year-old\n",
      "Generated 4 test responses\n",
      "Loading the saved reward model and tokenizer...\n",
      "Successfully loaded saved reward model and tokenizer.\n",
      "Scoring responses with the reward model...\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RESPONSES WITH THEIR SCORES:\n",
      "================================================================================\n",
      "\n",
      "Response 1 (Score: -0.2150)\n",
      "Prompt: 'Write a poem about artificial intelligence'\n",
      "Response: or AI.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Response 2 (Score: 0.5623)\n",
      "Prompt: 'Write a poem about artificial intelligence'\n",
      "Response: that you write on your iPad or iPhone.\n",
      "The first problem is not with the algorithm, but rather how it works in practice. In theory, an AI could learn what's written and say \"I'm doing this,\" which mig...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Response 3 (Score: -0.2909)\n",
      "Prompt: 'Explain how a car engine works to a 5-year-old'\n",
      "Response: 's perspective on the road.\n",
      " (Photo: Provided) Story Highlights A 6-foot tall, 3-inch wide red cross with blue lights makes an appearance in several of Los Angeles' most dangerous traffic stops for pe...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Response 4 (Score: 1.3989)\n",
      "Prompt: 'Explain how a car engine works to a 5-year-old'\n",
      "Response: .\n",
      ", we will take you through the process of building an electronic circuit that can be used as evidence in court or criminal case cases and show it is not just wrong but dangerous!\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "SUMMARY:\n",
      "Average Score: 0.3638\n",
      "Highest Score: 1.3989\n",
      "Lowest Score: -0.2909\n",
      "Score Range: 1.6898\n",
      "\n",
      "=== Evaluation Complete ===\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Function to generate responses using a separate generation model\n",
    "def generate_responses(prompt, num_responses=3):\n",
    "    \"\"\"Generate responses using a separate text generation model\"\"\"\n",
    "    try:\n",
    "        # Use a lightweight generation model\n",
    "        gen_model_name = \"gpt2\"  # You can also use \"microsoft/DialoGPT-medium\" or others\n",
    "        gen_model = AutoModelForCausalLM.from_pretrained(gen_model_name)\n",
    "        gen_tokenizer = AutoTokenizer.from_pretrained(gen_model_name)\n",
    "        \n",
    "        # Set pad token if not already set\n",
    "        if gen_tokenizer.pad_token is None:\n",
    "            gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
    "        \n",
    "        responses = []\n",
    "        for i in range(num_responses):\n",
    "            # Encode the prompt\n",
    "            inputs = gen_tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "            \n",
    "            # Generate response\n",
    "            torch.manual_seed(42 + i)  # Different seed for variety\n",
    "            with torch.no_grad():\n",
    "                outputs = gen_model.generate(\n",
    "                    inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.8,\n",
    "                    top_p=0.9,\n",
    "                    repetition_penalty=1.1,\n",
    "                    num_return_sequences=1,\n",
    "                    pad_token_id=gen_tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Decode the response\n",
    "            response = gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            # Remove the original prompt from the response\n",
    "            response = response[len(prompt):].strip()\n",
    "            responses.append(response)\n",
    "        \n",
    "        return responses\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating responses: {e}\")\n",
    "        # Fallback to manual responses\n",
    "        return [\n",
    "            f\"This is a sample response to: {prompt}\",\n",
    "            f\"Here's another response about: {prompt}\",\n",
    "            f\"A third perspective on: {prompt}\"\n",
    "        ]\n",
    "\n",
    "# Function to score responses with the reward model\n",
    "def score_responses(prompts, responses, reward_model, tokenizer):\n",
    "    scores = []\n",
    "    for prompt, response in zip(prompts, responses):\n",
    "        # Combine prompt and response for scoring\n",
    "        combined_text = f\"{prompt} {response}\"\n",
    "        inputs = tokenizer(combined_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = reward_model(**inputs)\n",
    "            scores.append(output.logits.item())\n",
    "    return scores\n",
    "\n",
    "# Testing the Reward Model\n",
    "print(\"=== Testing Reward Model ===\")\n",
    "\n",
    "# Generate new test responses\n",
    "test_prompts = [\n",
    "    \"Write a poem about artificial intelligence\",\n",
    "    \"Explain how a car engine works to a 5-year-old\"\n",
    "]\n",
    "\n",
    "print(\"Generating test responses...\")\n",
    "test_responses = []\n",
    "for prompt in test_prompts:\n",
    "    print(f\"Generating responses for: {prompt}\")\n",
    "    responses = generate_responses(prompt, num_responses=2)  # Generate 2 responses per prompt\n",
    "    for response in responses:\n",
    "        test_responses.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"answer\": response\n",
    "        })\n",
    "\n",
    "print(f\"Generated {len(test_responses)} test responses\")\n",
    "\n",
    "# Load the saved reward model for evaluation\n",
    "try:\n",
    "    print(\"Loading the saved reward model and tokenizer...\")\n",
    "    reward_model_path = \"reward_model\"\n",
    "    \n",
    "    # Check if model files exist\n",
    "    if os.path.exists(os.path.join(reward_model_path, \"config.json\")):\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(reward_model_path)\n",
    "        reward_tokenizer = AutoTokenizer.from_pretrained(reward_model_path)\n",
    "        print(\"Successfully loaded saved reward model and tokenizer.\")\n",
    "    else:\n",
    "        print(\"Warning: Saved model not found. Please ensure the reward model was trained and saved properly.\")\n",
    "        # Create a dummy model for demonstration\n",
    "        reward_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=1)\n",
    "        reward_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        print(\"Using a fresh DistilBERT model (not trained on your data).\")\n",
    "    \n",
    "    # Extract prompts and answers from test_responses\n",
    "    test_prompts_list = [item[\"prompt\"] for item in test_responses]\n",
    "    test_answers_list = [item[\"answer\"] for item in test_responses]\n",
    "    \n",
    "    print(\"Scoring responses with the reward model...\")\n",
    "    scores = score_responses(test_prompts_list, test_answers_list, reward_model, reward_tokenizer)\n",
    "    \n",
    "    # Plot reward scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(range(len(scores)), scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "    plt.title(\"Reward Scores for Generated Responses\")\n",
    "    plt.xlabel(\"Response Index\")\n",
    "    plt.ylabel(\"Reward Score\")\n",
    "    plt.xticks(range(len(scores)), [f\"R{i+1}\" for i in range(len(scores))])\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"reward_evaluation.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print responses with their scores\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RESPONSES WITH THEIR SCORES:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, (prompt, response, score) in enumerate(zip(test_prompts_list, test_answers_list, scores)):\n",
    "        print(f\"\\nResponse {i+1} (Score: {score:.4f})\")\n",
    "        print(f\"Prompt: '{prompt}'\")\n",
    "        print(f\"Response: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nSUMMARY:\")\n",
    "    print(f\"Average Score: {sum(scores)/len(scores):.4f}\")\n",
    "    print(f\"Highest Score: {max(scores):.4f}\")\n",
    "    print(f\"Lowest Score: {min(scores):.4f}\")\n",
    "    print(f\"Score Range: {max(scores) - min(scores):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during evaluation: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n=== Evaluation Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example code for loading and using the saved reward model:\n",
      "```python\n",
      "\n",
      "# Import necessary libraries\n",
      "import torch\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "# Load the saved model and tokenizer\n",
      "def load_reward_model(model_path):\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
      "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
      "    return model, tokenizer\n",
      "\n",
      "# Function to score a response with the reward model\n",
      "def score_response(prompt, response, model, tokenizer):\n",
      "    inputs = tokenizer(prompt, response, return_tensors=\"pt\", truncation=True, max_length=512)\n",
      "    with torch.no_grad():\n",
      "        output = model(**inputs)\n",
      "        score = output.logits.item()\n",
      "    return score\n",
      "\n",
      "# Load the model\n",
      "model_path = \"reward_model\"  # Path to your saved model\n",
      "model, tokenizer = load_reward_model(model_path)\n",
      "\n",
      "# Example usage\n",
      "prompt = \"Explain quantum computing to a 10-year-old child.\"\n",
      "responses = [\n",
      "    \"Quantum computers use weird physics to solve really hard problems super fast!\",\n",
      "    \"Quantum mechanics blah blah technical jargon blah blah.\"\n",
      "]\n",
      "\n",
      "# Score the responses\n",
      "for i, response in enumerate(responses):\n",
      "    score = score_response(prompt, response, model, tokenizer)\n",
      "    print(f\"Response {i+1} score: {score:.4f}\")\n",
      "\n",
      "```\n",
      "\\n✅ This notebook has now:\n",
      "1. Generated responses from a language model\n",
      "2. Added sample rankings (or used your manual rankings)\n",
      "3. Trained a reward model on these rankings\n",
      "4. Saved the model in the 'reward_model/' directory\n",
      "5. Evaluated the model on new responses\n",
      "6. Provided code for reusing the model in other applications\n",
      "\\n✅ The reward model is now ready to use in other applications!\n",
      "You can find it in the 'reward_model/' directory along with usage examples.\n"
     ]
    }
   ],
   "source": [
    "print(\"Example code for loading and using the saved reward model:\")\n",
    "print(\"```python\")\n",
    "print(\"\"\"\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "def load_reward_model(model_path):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to score a response with the reward model\n",
    "def score_response(prompt, response, model, tokenizer):\n",
    "    inputs = tokenizer(prompt, response, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        score = output.logits.item()\n",
    "    return score\n",
    "\n",
    "# Load the model\n",
    "model_path = \"reward_model\"  # Path to your saved model\n",
    "model, tokenizer = load_reward_model(model_path)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Explain quantum computing to a 10-year-old child.\"\n",
    "responses = [\n",
    "    \"Quantum computers use weird physics to solve really hard problems super fast!\",\n",
    "    \"Quantum mechanics blah blah technical jargon blah blah.\"\n",
    "]\n",
    "\n",
    "# Score the responses\n",
    "for i, response in enumerate(responses):\n",
    "    score = score_response(prompt, response, model, tokenizer)\n",
    "    print(f\"Response {i+1} score: {score:.4f}\")\n",
    "\"\"\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\\\n✅ This notebook has now:\")\n",
    "print(\"1. Generated responses from a language model\")\n",
    "print(\"2. Added sample rankings (or used your manual rankings)\")\n",
    "print(\"3. Trained a reward model on these rankings\")\n",
    "print(\"4. Saved the model in the 'reward_model/' directory\")\n",
    "print(\"5. Evaluated the model on new responses\")\n",
    "print(\"6. Provided code for reusing the model in other applications\")\n",
    "\n",
    "print(\"\\\\n✅ The reward model is now ready to use in other applications!\")\n",
    "print(f\"You can find it in the 'reward_model/' directory along with usage examples.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
